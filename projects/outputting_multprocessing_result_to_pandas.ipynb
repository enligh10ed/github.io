{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputting the result of multiprocessing to a `pandas` dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` provides a [high-performance, easy-to-use data structures and data analysis tools](http://pandas.pydata.org/) for Python programming. However, using `pandas` with multiprocessing can be a challenge. In his stackoverflow [post](http://stackoverflow.com/a/30395396/3594865), Mike McKerns, nicely summarizes why this is so. He says:\n",
    "\n",
    "> ### You are asking multiprocessing (or other python parallel modules) to output to a data structure that they don't directly output to.\n",
    "\n",
    "\n",
    "This tutorial demonstrates a straightforward workaround where you can return a list of lists from multiprocessing and then convert that to a `pandas` data frame. While you're not getting a `pandas` data frame straight from your threads, you still get a `pandas` data frame at the end. Hooray!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume you're performing a compute intensive operation on multiple data frames. At the end of the operation, you want to merge the data frames. \n",
    "\n",
    "Here's what the workflow might look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 5)\n",
      "--- 79.52008247375488 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://www.math.uah.edu/stat/data/Fisher.csv\"\n",
    "response = urlopen(url)\n",
    "df = pd.read_csv(response)\n",
    "results = []\n",
    "\n",
    "def processData(df):  \n",
    "    \"\"\"Does some compute intensive operation on the data frame.\n",
    "       Returns a list.\"\"\"\n",
    "       \n",
    "    for i in range(100000):\n",
    "        df = df * 1.0\n",
    "    return df.values.tolist()\n",
    "\n",
    "def collect_results(result):\n",
    "    \"\"\"Uses apply_async's callback to setup up a separate Queue for each process\"\"\"\n",
    "    results.extend(result)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()  \n",
    "    \n",
    "    # Repeats the compute intensive operation on 10 data frames concurrently\n",
    "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
    "    for i in range(10): \n",
    "        pool.apply_async(processData, args=(df, ), callback=collect_results)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Converts list of lists to a data frame\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df.shape)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key parts of the parallel process above are `df.values.tolist()` and `callback=collect_results`. With `df.values.tolist()`, we're converting the processed data frame to a list which is a data structure we can directly output from multiprocessing. With `callback=collect_results`, we're using the multiprocessing's `callback` functionality to setup up a separate queue for each process. This prevents deadlock or other synchronization problems on the shared resource, `results`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what a serial version might look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 5)\n",
      "--- 134.60705041885376 seconds ---\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.math.uah.edu/stat/data/Fisher.csv\"\n",
    "response = urlopen(url)\n",
    "df = pd.read_csv(response)\n",
    "    \n",
    "results = []\n",
    "\n",
    "\n",
    "def processData(df):  \n",
    "    \"\"\"Performs some compute intensive operation on the data frame.\n",
    "       Returns a list.\"\"\"\n",
    "       \n",
    "    for i in range(100000):\n",
    "        df = df * 1.0\n",
    "    return df\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Repeats the compute intensive operation on 10 data frames serially\n",
    "    for i in range(10):\n",
    "        results.append(processData(df))\n",
    "    final = pd.concat(results, axis=0, ignore_index=True)    \n",
    "\n",
    "    print(final.shape)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using multiprocessing, we're able to reduce the processing time significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine using this workflow if you're downloading fairly large datasets from an online source. Instead of waiting for each download to complete before moving to the next download, you can parallelize the process so that multiple downloads are happening simultaneously. \n",
    "\n",
    "Your output from running this script will likely differ. Below is my system's information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 :: Anaconda 4.1.1 (64-bit)\n",
      "Linux-3.13.0-74-generic-x86_64-with-debian-jessie-sid\n",
      "cpu cores: 8\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "import platform\n",
    "print(platform.platform())\n",
    "print(\"cpu cores: {0}\".format(multiprocessing.cpu_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I couldn't get this notebook to run on Windows 10. It appears that Windows 10 has trouble running a jupyter notebook containing multiprocessing code. I had to spurn up an Amazon EC2 Linux instance (m4.2xlarge) and it ran the notebook without any problem. If you successfully ran the notebook on Windows 10, I like to hear what you did!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
