{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where the data science jobs are? (Part 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we're going to retrieve and clean salary data for data science related jobs. In the next installment, [Where the data science jobs are (part 2)](https://github.com/sedeh/github.io/blob/master/projects/where_the_data_science_related_jobs_are_part2.ipynb), we'll analyze the data and gain insights into data science related jobs. \n",
    "\n",
    "We're going to source the data from the U.S. Department of Labor (DOL). When a U.S. company wants to hire a non-U.S. worker, the company is required to file a work visa (H1B) or permanent residency (greencard) application with DOL. As part of the application, the company must disclose how much it will pay the non-U.S. worker. The company is also required to disclose the `Prevailing Wage` for the job, i.e. how much U.S. workers in similar positions are being paid. The idea is to ensure that hiring non-U.S. workers does not negatively affect the job market for U.S. workers. \n",
    "\n",
    "We'll take advantage of this publicly available data to explore information about data science related jobs. For this analysis, we're going to consider H1B work visa applications from 2002 to 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the packages we'll need and set things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# For now, let's turn off panda's warning\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "# Snippets like this will be used thruout the code to get idea of program runtime\n",
    "total_time = time.time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get some system info to help with your troubleshooting in the likely event you're using a different environment from the one I used in developing and testing this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 :: Anaconda 4.1.1 (64-bit)\n",
      "Linux-3.13.0-74-generic-x86_64-with-debian-jessie-sid\n",
      "cpu cores: 8\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(platform.platform())\n",
    "print(\"cpu cores: {0}\".format(mp.cpu_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python script to download the data is [here](https://github.com/sedeh/github.io/blob/master/resources/h1bdata_download.py). The data is **561MB**. Let's read it in and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "h1bdataDF has 3886329 rows and 14 columns\n",
      "\n",
      "\n",
      "        Submitted_Date      Case_Number                      Employer_Name  \\\n",
      "0    10/1/2006 0:03:29  I-06274-2831343     Solar Link International, Inc.   \n",
      "1    10/1/2006 1:47:12  I-06274-2831345           Friendship Corner School   \n",
      "2   10/3/2006 13:18:55  I-06274-2831347  The Underground World Corporation   \n",
      "3  10/23/2006 20:11:19  I-06274-2831349                       Solutia Inc.   \n",
      "4    10/1/2006 8:07:17  I-06274-2831351        Trustmark Insurance Company   \n",
      "\n",
      "  Employer_City Employer_State Employer_Postal_Code  \\\n",
      "0       Ontario             CA                91761   \n",
      "1      Bartlett             IL                60103   \n",
      "2     Royal Oak             MI                48067   \n",
      "3     St. Louis             MO                63141   \n",
      "4   Lake Forest             IL                60045   \n",
      "\n",
      "                                   Job_Title Approval_Status  Wage_Rate  \\\n",
      "0                                 Accountant       Certified  $36000.00   \n",
      "1                       Kindergarten Teacher       Certified  $23500.00   \n",
      "2                        Industrial Designer       Certified  $73750.00   \n",
      "3  OFFSC Supervisor & Manufacturing Engineer       Certified  $68500.00   \n",
      "4                             System Analyst       Certified  $62738.00   \n",
      "\n",
      "  Wage_Rate_Unit Part_Time    Work_City Work_State Prevailing_Wage  \n",
      "0           Year         N      Ontario         CA       $36000.00  \n",
      "1           Year         N     Bartlett         IL       $23480.00  \n",
      "2           Year         N    Royal Oak         MI       $59072.00  \n",
      "3           Year         N  Springfield         MO       $65520.00  \n",
      "4           Year         N  Lake Forest         IL       $52478.00  \n"
     ]
    }
   ],
   "source": [
    "h1bdataDF = pickle.load(open('h1bdataDF.pkl', 'rb'))\n",
    "print(\"\\n\")\n",
    "print(\"h1bdataDF has {0} rows and {1} columns\".format(h1bdataDF.shape[0], h1bdataDF.shape[1]))\n",
    "print(\"\\n\")\n",
    "print(h1bdataDF.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've about 4 million records. Let's remove any duplicate application that may be in the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "h1bdataDF2 has 3788868 rows and 14 columns\n"
     ]
    }
   ],
   "source": [
    "h1bdataDF2 = h1bdataDF.drop_duplicates(['Case_Number'], keep='last')\n",
    "print(\"\\n\")\n",
    "print(\"h1bdataDF2 has {0} rows and {1} columns\".format(h1bdataDF2.shape[0], h1bdataDF2.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the available options for `Approval_Status` field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Certified', 'Denied', 'Hold', 'Pending', None, 'DENIED',\n",
       "       'CERTIFIED', 'WITHDRAWN', 'CERTIFIED-WITHDRAWN',\n",
       "       'PENDING QUALITY AND COMPLIANCE REVIEW - UNASSIGNED', 'REJECTED',\n",
       "       'INVALIDATED'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1bdataDF.Approval_Status.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purpose, it makes sense to keep only applications that were `certified`. A `denied` application for example does not give you a lot of confidence about the quality of the application. Before we do that, we need to first remove rows with `NA`s in the `Approval_Status` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filteredDF: (3788867, 14)\n",
      "certifiedDF: (3550156, 14)\n"
     ]
    }
   ],
   "source": [
    "filteredDF = h1bdataDF2.dropna(subset=['Approval_Status'])\n",
    "print(\"filteredDF: {0}\".format(filteredDF.shape))\n",
    "certifiedDF = filteredDF[filteredDF.Approval_Status.str.lower().str.contains('certified')]\n",
    "print(\"certifiedDF: {0}\".format(certifiedDF.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize data science related jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to select applications filed for data science related jobs. I chose the selection criteria based on my view and research of the data science field. But this isn't perfect and feel free to swap in your own criteria :).\n",
    "\n",
    "First, let's make sure `Job_Title` column contains only `string`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Job_Title\n",
      "435289   66007.9\n",
      "457835     75483\n",
      "473113     27.89\n",
      "494274     71400\n",
      "546898    541613\n"
     ]
    }
   ],
   "source": [
    "rows_with_strings  = certifiedDF[[\"Job_Title\"]].apply(\n",
    "       lambda row : \n",
    "          any([ isinstance(e, str) for e in row ])\n",
    "       , axis=1) \n",
    "    \n",
    "df_with_no_strings = certifiedDF[[\"Job_Title\"]][~rows_with_strings]\n",
    "print(df_with_no_strings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems some job titles were entered as `float`s. Let's clean out all `float` job titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3550104, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "certifiedDF2 = certifiedDF[rows_with_strings]\n",
    "certifiedDF2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can standardize the job titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsJobsDF shape: (209240, 15)\n",
      "Time to select data science related jobs and standardize job titles --- 354.9449350833893 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "titles_dict = {'data analyst': 'data analyst', 'business analyst':'business analyst',\n",
    "               'informatic': 'informatics', 'operations research':'operations research',\n",
    "               'data scien': 'data scientist', 'intelligence': 'business intelligence', \n",
    "               'analytics': 'business intelligence', 'data consultan': 'business intelligence', \n",
    "               'data manage': 'business intelligence','reporting & analysis': 'business intelligence',\n",
    "               'data min': 'business intelligence','reporting and analysis': 'business intelligence',\n",
    "               'database analyst':'data architect/engineer', 'data architect': 'data architect/engineer',\n",
    "               'data engineer': 'data architect/engineer','data warehous': 'data architect/engineer',\n",
    "               'database admin': 'data architect/engineer', 'datawarehous': 'data architect/engineer', \n",
    "               'data model': 'data architect/engineer','etl developer':'data architect/engineer', \n",
    "               'market research analyst': 'market analyst', 'market analyst': 'market analyst', \n",
    "               'customer insight': 'market analyst', 'market insight': 'market analyst',\n",
    "               'consumer insight': 'market analyst', 'marketing insight': 'market analyst',\n",
    "               'insights analyst': 'market analyst', 'analytic insight': 'market analyst',\n",
    "               'strategy and insight': 'market analyst', 'mkt. res. analyst': 'market analyst',\n",
    "               'marketing & insights': 'market analyst', 'global insights': 'market analyst',\n",
    "               'insights & analytic':'market analyst', 'strategy and insight': 'market analyst',\n",
    "               'marketing & insight': 'market analyst','insights & analytic':'market analyst',\n",
    "               'health care analyst':'market analyst', 'healthcare analyst': 'market analyst',\n",
    "               'quantitative analyst':'market analyst', 'financial analyst':'market analyst',\n",
    "               'marketing analyst':'market analyst', 'management analyst':'market analyst', \n",
    "               'business development': 'market analyst', 'consumer insight':'market analyst',\n",
    "               'analytic insight':'market analyst', 'data strateg':'market analyst',\n",
    "               'decision scien':'data scientist'}\n",
    "\n",
    "titles_list = []\n",
    "\n",
    "for index, row in certifiedDF2.iterrows():\n",
    "    title = row.Job_Title.lower()\n",
    "    currentTitle = [val for key,val in titles_dict.items() if key in title]\n",
    "    if currentTitle:\n",
    "        titles_list.append(currentTitle[0])\n",
    "    else:\n",
    "        titles_list.append(\"non data science related\")\n",
    "        \n",
    "certifiedDF2[\"Job_Title_revised\"] = titles_list\n",
    "\n",
    "# Filter out non data science jobs\n",
    "dsJobsDF = certifiedDF2[certifiedDF2.Job_Title_revised != \"non data science related\"]\n",
    "dsJobsDF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"dsJobsDF shape: {0}\".format(dsJobsDF.shape))\n",
    "print(\"Time to select data science related jobs and standardize job titles --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at `Wage_Rate` column and see if any standardization is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year' 'hour' 'month' 'week' '2 weeks' 'bi-weekly' 'yr' 'hr' 'wk' 'mth'\n",
      " 'bi']\n"
     ]
    }
   ],
   "source": [
    "print(dsJobsDF.Wage_Rate_Unit.str.lower().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wage rates range from hourly, weekly, bi-weekly, monthly to yearly. Let's standardize to yearly. But we need to make sure that the wage columns are clean and do not contain special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$48000.00' '$47359.00' '$48600.00' ..., '147014 -' '47819.00 -' '64501 -']\n",
      "['$40706.00' '$37149.00' '$37669.00' ..., 117915.0 43.54 147014.0]\n"
     ]
    }
   ],
   "source": [
    "print(dsJobsDF.Wage_Rate.unique())\n",
    "print(dsJobsDF.Prevailing_Wage.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the wage columns contains some special characters such as dollar sign and hyphen. Let's clean those out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209240, 15)\n",
      "Time to clean wage columns --- 1.0070388317108154 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dsJobsDF2 = dsJobsDF\n",
    "\n",
    "# Ensure the columns are of string before applying regex\n",
    "dsJobsDF2['Wage_Rate'] = dsJobsDF2['Wage_Rate'].astype(str)\n",
    "dsJobsDF2['Prevailing_Wage'] = dsJobsDF2['Prevailing_Wage'].astype(str)\n",
    "\n",
    "# Remove '$' and '-' signs from Wage_Rate column\n",
    "dsJobsDF2['Wage_Rate'] = dsJobsDF2['Wage_Rate'].map(lambda x: re.sub('\\$|-.*', '', x))\n",
    "# Remove '$' and '-' signs from Prevailing_Wage column\n",
    "dsJobsDF2['Prevailing_Wage'] = dsJobsDF2['Prevailing_Wage'].map(lambda x: re.sub('\\$|-.*', '', x))\n",
    "        \n",
    "# Convert the cleaned wage columns to float\n",
    "dsJobsDF2[['Wage_Rate','Prevailing_Wage']] = dsJobsDF2[['Wage_Rate','Prevailing_Wage']].apply(pd.to_numeric)\n",
    "print(dsJobsDF2.shape)\n",
    "print(\"Time to clean wage columns --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the wage columns look like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.07060000e+04   3.71490000e+04   3.76690000e+04 ...,   1.17915000e+05\n",
      "   4.35400000e+01   1.47014000e+05]\n"
     ]
    }
   ],
   "source": [
    "print(dsJobsDF2.Prevailing_Wage.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.80000000e+04   4.73590000e+04   4.86000000e+04 ...,   4.35400000e+01\n",
      "   5.05000000e+03   1.47014000e+05]\n"
     ]
    }
   ],
   "source": [
    "print(dsJobsDF2.Wage_Rate.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we successfully removed all special characters in the wage columns. Now, finally, we can standardize wage to yearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to standardize salary --- 27.00463628768921 seconds ---\n",
      "dsJobsDF3: (209232, 17)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "salary_offered = []\n",
    "salary_prevailing = []\n",
    "for index, row in dsJobsDF2.iterrows():\n",
    "    if row.Wage_Rate_Unit.lower()[0] == 'y':\n",
    "        salary_offered.append(row.Wage_Rate)\n",
    "        salary_prevailing.append(row.Prevailing_Wage)\n",
    "    elif row.Wage_Rate_Unit.lower()[0] == 'm':\n",
    "        salary_offered.append(row.Wage_Rate * 12)\n",
    "        salary_prevailing.append(row.Prevailing_Wage * 12)\n",
    "    elif row.Wage_Rate_Unit.lower() in ['bi', '2 weeks', 'bi-weekly']:\n",
    "        salary_offered.append(row.Wage_Rate * 26) # Assumes 52 work wks/yr\n",
    "        salary_prevailing.append(row.Prevailing_Wage * 26)\n",
    "    elif row.Wage_Rate_Unit.lower()[0] == 'w':\n",
    "        salary_offered.append(row.Wage_Rate * 52) # Assumes 52 work wks/yr\n",
    "        salary_prevailing.append(row.Prevailing_Wage * 52)\n",
    "    else:\n",
    "        salary_offered.append(row.Wage_Rate * 2080) # This handles the hourly rates. Assumes 52 work wks of 40 hrs/wk\n",
    "        salary_prevailing.append(row.Prevailing_Wage * 2080)\n",
    "\n",
    "dsJobsDF3 = dsJobsDF2\n",
    "dsJobsDF3[\"Salary_Offered\"] = salary_offered\n",
    "dsJobsDF3[\"Salary_Prevailing\"] = salary_prevailing\n",
    "\n",
    "# Let's remove any rows without salary Wage information\n",
    "dsJobsDF3 = dsJobsDF3.dropna(subset=['Salary_Offered'])\n",
    "dsJobsDF3 = dsJobsDF3.dropna(subset=['Salary_Prevailing'])\n",
    "\n",
    "print(\"Time to standardize salary --- %s seconds ---\" % (time.time() - start_time))\n",
    "print('dsJobsDF3: {0}'.format(dsJobsDF3.shape))\n",
    "dsJobsDF3.reset_index(drop=True, inplace=True) # Resets row index so they start from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some salary information seems questionable. We've many wages that seem unbelievably high. In the data frame below, row **2801** has hourly rate of **4330560.0**. At the other extreme, we've annual salary of **23.37** for row **24343**. These values are likely entry errors and we should remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Submitted_Date      Case_Number  \\\n",
      "368    10/9/2006 17:12:11  I-06282-2844381   \n",
      "408   10/10/2006 14:16:26  I-06283-2845897   \n",
      "1865  11/15/2006 19:05:54  I-06319-2902685   \n",
      "2636   12/7/2006 18:24:27  I-06341-2931261   \n",
      "2801  12/13/2006 14:54:25  I-06347-2938439   \n",
      "\n",
      "                             Employer_Name Employer_City Employer_State  \\\n",
      "368        JLT AEROSPACE(NORTH AMERICA)INC        LATHAM             NY   \n",
      "408               Anant B. Soni M.D., P.A.       Raleigh             NC   \n",
      "1865  Kismet Real Estate Investments, Inc.      Moorpark             CA   \n",
      "2636          LG Electronics Alabama, Inc.    Huntsville             AL   \n",
      "2801                       baik kwang corp     carlstadt             NJ   \n",
      "\n",
      "     Employer_Postal_Code                                       Job_Title  \\\n",
      "368                 12110  Director of International Business Development   \n",
      "408                 27612                          Database Administrator   \n",
      "1865                93021                              Management Analyst   \n",
      "2636                35824                              Management Analyst   \n",
      "2801                07072                               financial analyst   \n",
      "\n",
      "     Approval_Status   Wage_Rate Wage_Rate_Unit Part_Time     Work_City  \\\n",
      "368        Certified  20000000.0           Year         N  Coral Gables   \n",
      "408        Certified     27000.0           Hour         Y       Raleigh   \n",
      "1865       Certified     48000.0           Hour         N      Moorpark   \n",
      "2636       Certified   5399992.0           Year         N    Huntsville   \n",
      "2801       Certified      2082.0           Hour         Y     carlstadt   \n",
      "\n",
      "     Work_State  Prevailing_Wage        Job_Title_revised  Salary_Offered  \\\n",
      "368          NY        5243700.0           market analyst      20000000.0   \n",
      "408          NC          17011.2  data architect/engineer      56160000.0   \n",
      "1865         CA          47154.0           market analyst      99840000.0   \n",
      "2636         AL          52146.0           market analyst       5399992.0   \n",
      "2801         NJ           2082.0           market analyst       4330560.0   \n",
      "\n",
      "      Salary_Prevailing  \n",
      "368           5243700.0  \n",
      "408          35383296.0  \n",
      "1865         98080320.0  \n",
      "2636            52146.0  \n",
      "2801          4330560.0  \n",
      "\n",
      "\n",
      "\n",
      "            Submitted_Date         Case_Number  \\\n",
      "24343  2009-07-16 00:00:00  I-200-09196-549892   \n",
      "24379  2009-07-16 00:00:00  I-200-09197-448940   \n",
      "25985  2009-09-04 00:00:00  I-200-09247-442999   \n",
      "26357  2009-09-15 00:00:00  I-200-09258-663062   \n",
      "26547  2009-09-18 00:00:00  I-200-09261-887645   \n",
      "\n",
      "                         Employer_Name Employer_City Employer_State  \\\n",
      "24343                   TRADEPRO, INC.   NORTH MIAMI             FL   \n",
      "24379        BAYLOR RESEARCH INSTITUTE        DALLAS             TX   \n",
      "25985               COMPUTER AID, INC.     ALLENTOWN             PA   \n",
      "26357  KOMAL UDYOG INTERNATIONAL, INC.   CHILLICOTHE             IL   \n",
      "26547    FIVE DIAMOND COLD STORAGE INC        DELANO             CA   \n",
      "\n",
      "      Employer_Postal_Code                                 Job_Title  \\\n",
      "24343                33181  MARKETING & BUSINESS DEVELOPMENT MANAGER   \n",
      "24379                75204                RESEARCH FINANCIAL ANALYST   \n",
      "25985                18104                            DATA ARCHITECT   \n",
      "26357                61523                          BUSINESS ANALYST   \n",
      "26547                93215                        MANAGEMENT ANALYST   \n",
      "\n",
      "      Approval_Status  Wage_Rate Wage_Rate_Unit Part_Time      Work_City  \\\n",
      "24343       CERTIFIED      23.37           Year         Y    NORTH MIAMI   \n",
      "24379       CERTIFIED      26.45           Year         Y         DALLAS   \n",
      "25985       CERTIFIED      67.50           Year         Y  WASHINTON, DC   \n",
      "26357       CERTIFIED      25.00           Year         Y    CHILLICOTHE   \n",
      "26547       CERTIFIED      25.40           Year         Y         DELANO   \n",
      "\n",
      "      Work_State  Prevailing_Wage        Job_Title_revised  Salary_Offered  \\\n",
      "24343         FL          42533.4           market analyst           23.37   \n",
      "24379         TX          50523.0           market analyst           26.45   \n",
      "25985         DC          79477.0  data architect/engineer           67.50   \n",
      "26357         IL          36629.0         business analyst           25.00   \n",
      "26547         CA          46197.0           market analyst           25.40   \n",
      "\n",
      "       Salary_Prevailing  \n",
      "24343            42533.4  \n",
      "24379            50523.0  \n",
      "25985            79477.0  \n",
      "26357            36629.0  \n",
      "26547            46197.0  \n"
     ]
    }
   ],
   "source": [
    "print(dsJobsDF3[dsJobsDF3.Salary_Offered > 1000000].head())\n",
    "print(\"\\n\\n\")\n",
    "print(dsJobsDF[dsJobsDF.Salary_Offered < 100].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208610, 17)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's keep salaries that are greater than 20,000 but less than 1.5 million\n",
    "dsJobsDF3 = dsJobsDF3[(dsJobsDF3.Salary_Offered > 20000) & (dsJobsDF3.Salary_Offered < 1500000)]\n",
    "dsJobsDF3 = dsJobsDF3[(dsJobsDF3.Salary_Prevailing > 20000) & (dsJobsDF3.Salary_Prevailing < 1500000)]\n",
    "dsJobsDF3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add price parity information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to adjust salary to reflect regional price parity (inflation). Let's get regional inflation data from the Bureau of Economic Analysis, www.bea.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parityDF shape: (209240, 17)\n",
      "Time to read and clean BEA data --- 0.8698716163635254 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "url = 'https://www.bea.gov/newsreleases/regional/rpp/2015/xls/rpp0615.xlsx'\n",
    "response = urlopen(url)\n",
    "parityDF = pd.read_excel(response, skiprows=3)\n",
    "parityDF.drop(parityDF.columns[[1,2,3,4,5,6,9]], axis=1, inplace=True)\n",
    "parityDF.columns = [\"State\", \"2012\", \"2013\"]\n",
    "parityDF['Price_Deflator'] = (parityDF['2012'] + parityDF['2013']) / 2 \n",
    "parityDF.drop(parityDF.columns[[1,2]], axis=1, inplace=True)\n",
    "\n",
    "print(\"parityDF shape: {0}\".format(dsJobsDF.shape))\n",
    "print(\"Time to read and clean BEA data --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the price parity data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State  Price_Deflator\n",
      "0     Alabama           93.65\n",
      "1      Alaska          113.15\n",
      "2     Arizona          103.80\n",
      "3    Arkansas           93.30\n",
      "4  California          120.00\n"
     ]
    }
   ],
   "source": [
    "print(parityDF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to merge the price parity information with the salary data. The common column between both data frames is `State` and we should take a look at the `State` columns in both data frames to make sure they match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TX' 'AL' 'FL' 'IL' 'PA' 'NY' 'MO' 'NJ' 'LA' 'CA' 'OK' 'WA' 'MA' 'WI' 'CO'\n",
      " 'MN' 'NC' 'VA' 'GA' 'MD' 'OH' 'VI' 'TN' 'CT' 'MI' 'AZ' 'RI' 'NH' 'DC' 'NM'\n",
      " 'NV' 'SC' 'HI' 'KY' 'NE' 'DE' 'ID' 'AR' 'MT' 'OR' 'IN' 'IA' 'ME' 'KS' 'AK'\n",
      " 'SD' 'MS' 'PR' 'ND' 'UT' 'GU' 'WY' 'VT' 'WV' 'FM' 'PW' 'AS' 'MH' 'MP']\n"
     ]
    }
   ],
   "source": [
    "print(dsJobsDF3.Work_State.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alabama' 'Alaska' 'Arizona' 'Arkansas' 'California' 'Colorado'\n",
      " 'Connecticut' 'Delaware' 'District of Columbia' 'Florida' 'Georgia'\n",
      " 'Hawaii' 'Idaho' 'Illinois' 'Indiana' 'Iowa' 'Kansas' 'Kentucky'\n",
      " 'Louisiana' 'Maine' 'Maryland' 'Massachusetts' 'Michigan' 'Minnesota'\n",
      " 'Mississippi' 'Missouri' 'Montana' 'Nebraska' 'Nevada' 'New Hampshire'\n",
      " 'New Jersey' 'New Mexico' 'New York' 'North Carolina' 'North Dakota'\n",
      " 'Ohio' 'Oklahoma' 'Oregon' 'Pennsylvania' 'Rhode Island' 'South Carolina'\n",
      " 'South Dakota' 'Tennessee' 'Texas' 'Utah' 'Vermont' 'Virginia'\n",
      " 'Washington' 'West Virginia' 'Wisconsin' 'Wyoming' nan 'Maximum' 'Minimum'\n",
      " 'Range' 'Source: U.S. Bureau of Economic Analysis']\n"
     ]
    }
   ],
   "source": [
    "print(parityDF.State.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice two things. First, the price parity dataset contains extraneous rows named `maximum`, `minimum`, `range` and `nan`. Second, the two datasets use different formats for state names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's address the second issue and convert state abbreviation in the work visa dataset to state full name using the list [here](https://raw.githubusercontent.com/sedeh/github.io/master/resources/states.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsJobsDF3: (208595, 17)\n"
     ]
    }
   ],
   "source": [
    "dsJobsDF3 = dsJobsDF3.dropna(subset=['Employer_State'])\n",
    "print('dsJobsDF3: {0}'.format(dsJobsDF3.shape))\n",
    "response = urlopen('https://raw.githubusercontent.com/sedeh/github.io/master/resources/states.txt')\n",
    "lines = response.readlines()\n",
    "state_names_dict = {}\n",
    "for line in lines:\n",
    "    state_code, state_name = line.decode().split(\":\")\n",
    "    state_names_dict[state_code.strip()] = state_name.strip()\n",
    "\n",
    "state_names_list = []\n",
    "\n",
    "for index, row in dsJobsDF3.iterrows():\n",
    "    try:\n",
    "        state_names_list.append(state_names_dict[row.Work_State])\n",
    "    except KeyError:\n",
    "        state_names_list.append(state_names_dict[row.Employer_State])\n",
    "dsJobsDF4 = dsJobsDF3\n",
    "dsJobsDF4[\"Work_State_Code\"] = dsJobsDF4.Work_State\n",
    "dsJobsDF4[\"Work_State\"] = state_names_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now remove the extraneous values in the price parity data, `maximum`, `minimum`, `range`, and `nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parityDF2: (51, 2)\n"
     ]
    }
   ],
   "source": [
    "parityDF2 = parityDF[:-5]\n",
    "parityDF2 = parityDF2.dropna(subset=['State'])\n",
    "print('parityDF2: {0}'.format(parityDF2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge the parity and work visa datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208595, 20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsJobsDF5 = dsJobsDF4.merge(parityDF2,how='left', left_on='Work_State', right_on='State')\n",
    "dsJobsDF5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to use the price parity information to adjust salary to reflect regional cost of living differences. Notice that not all states have price parity information. You can see this more clearly below with the presence of `nan` in the `Price_Deflator` column. We should replace those `nan` values with **100.0** so the salary for those states will remain unchanged after adjusting for cost of living differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan present\n",
      "[ 102.9    93.65  105.35  107.5   104.95  122.75   94.9   122.1    97.3\n",
      "  120.     95.8   110.    114.1    99.2   108.65  104.05   97.65  109.8\n",
      "   97.9   118.1    95.35     nan   96.6   115.95  100.6   103.8   104.8\n",
      "  112.75  101.3   104.9   124.35   94.75   96.4   108.25   99.3    93.3\n",
      "  100.3   105.15   97.35   96.05  104.4    96.65  113.15   93.95   92.3\n",
      "   97.1   103.6   102.3   107.     94.3 ]\n",
      "\n",
      "\n",
      "nan replaced with 100.0\n",
      "[ 102.9    93.65  105.35  107.5   104.95  122.75   94.9   122.1    97.3\n",
      "  120.     95.8   110.    114.1    99.2   108.65  104.05   97.65  109.8\n",
      "   97.9   118.1    95.35  100.     96.6   115.95  100.6   103.8   104.8\n",
      "  112.75  101.3   104.9   124.35   94.75   96.4   108.25   99.3    93.3\n",
      "  100.3   105.15   97.35   96.05  104.4    96.65  113.15   93.95   92.3\n",
      "   97.1   103.6   102.3   107.     94.3 ]\n"
     ]
    }
   ],
   "source": [
    "print('nan present')\n",
    "print(dsJobsDF5.Price_Deflator.unique())\n",
    "print(\"\\n\")\n",
    "where_are_NaNs = np.isnan(dsJobsDF5.Price_Deflator)\n",
    "dsJobsDF5.Price_Deflator[where_are_NaNs] = 100.0\n",
    "print('nan replaced with 100.0')\n",
    "print(dsJobsDF5.Price_Deflator.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to adjust salary with the price parity information. This isn't perfect because we're using price parity information from **2012 and 2013** to adjust salary information from **2002 to 2016**. However, it seems likely that inflation shows consistent trend from year to year among states. For e.g., New York has been more expensive to live in than Iowa for as long as we can remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to adjust salary for inflation --- 43.80738282203674 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "salary_offered_adjusted = []\n",
    "salary_prevailing_adjusted = []\n",
    "\n",
    "for index, row in dsJobsDF5.iterrows():\n",
    "    salary_offered_adjusted.append(row.loc['Salary_Offered'] / (row.loc['Price_Deflator'] / 100))\n",
    "    salary_prevailing_adjusted.append(row.loc['Salary_Prevailing'] / (row.loc['Price_Deflator'] / 100))\n",
    "    \n",
    "dsJobsDF6 = dsJobsDF5\n",
    "dsJobsDF6[\"Offered_Salary_Adjusted\"] = salary_offered_adjusted\n",
    "dsJobsDF6[\"Prevailing_Salary_Adjusted\"] = salary_prevailing_adjusted\n",
    "\n",
    "print(\"Time to adjust salary for inflation --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add population information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply knowing that California has **30,000** data science related jobs compared to say **3500** for Alaska really does not tell us all that much. Relative truth is better where we get a sense of the number of H1B jobs per capita. So let's add population information from census.gov to help us do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         State  Census_2015\n",
      "0     .Alabama    4858979.0\n",
      "1      .Alaska     738432.0\n",
      "2     .Arizona    6828065.0\n",
      "3    .Arkansas    2978204.0\n",
      "4  .California   39144818.0\n",
      "\n",
      "\n",
      "        State  Census_2015\n",
      "0     Alabama    4858979.0\n",
      "1      Alaska     738432.0\n",
      "2     Arizona    6828065.0\n",
      "3    Arkansas    2978204.0\n",
      "4  California   39144818.0\n",
      "\n",
      "\n",
      "censusDF: (59, 2)\n",
      "Time to read and clean census data --- 0.2861607074737549 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "url = 'http://www.census.gov/popest/data/state/totals/2015/tables/NST-EST2015-01.xlsx'\n",
    "response = urlopen(url)\n",
    "xl = pd.ExcelFile(response)\n",
    "sheet_names = xl.sheet_names\n",
    "DF = xl.parse(sheet_names[0], skiprows=8)\n",
    "DF.columns = [\"State\", \"Census\", \"Estimates Base\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"Census_2015\"]\n",
    "censusDF = DF[[\"State\", \"Census_2015\"]]\n",
    "censusDF = censusDF.iloc[0:51,]\n",
    "print(censusDF.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Remove dot sign from the State column\n",
    "censusDF['State'] = censusDF['State'].map(lambda x: re.sub('\\.', '', x))\n",
    "censusDF['State'] = censusDF['State'].str.title()\n",
    "\n",
    "# Add population of US territories not included in the data source\n",
    "temp = pd.DataFrame([['Puerto Rico', 3474182],\n",
    "                   ['Guam', 159358],\n",
    "                   ['Virgin Islands', 106405],\n",
    "                   ['American Samoa', 55519],\n",
    "                   ['Northern Mariana Islands', 53883],\n",
    "                   ['Palau', 20918],\n",
    "                   ['Federated States Of Micronesia', 103549],\n",
    "                   ['Marshall Islands', 52634]], columns=[\"State\", \"Census_2015\"])\n",
    "censusDF = censusDF.append(temp, ignore_index=True)\n",
    "print(censusDF.head())\n",
    "print(\"\\n\")\n",
    "print(\"censusDF: {0}\".format(censusDF.shape))\n",
    "print(\"Time to read and clean census data --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the census data to the job data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208595, 24)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsJobsDF7 = dsJobsDF6.merge(censusDF,how='left', left_on='Work_State', right_on='State')\n",
    "dsJobsDF7.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can drop columns we clearly don't need. Here are the columns we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Submitted_Date', 'Case_Number', 'Employer_Name', 'Employer_City',\n",
       "       'Employer_State', 'Employer_Postal_Code', 'Job_Title',\n",
       "       'Approval_Status', 'Wage_Rate', 'Wage_Rate_Unit', 'Part_Time',\n",
       "       'Work_City', 'Work_State', 'Prevailing_Wage', 'Job_Title_revised',\n",
       "       'Salary_Offered', 'Salary_Prevailing', 'Work_State_Code', 'State_x',\n",
       "       'Price_Deflator', 'Offered_Salary_Adjusted',\n",
       "       'Prevailing_Salary_Adjusted', 'State_y', 'Census_2015'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsJobsDF7.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsJobsDF_final = dsJobsDF7.drop(dsJobsDF7.columns[[1,3,4,5,6,7,8,9,10,13,15,16,18,22]], axis=1)\n",
    "dsJobsDF_final.rename(columns={'Job_Title_revised':'Job_Category'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our final dataset and the retained columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Submitted_Date', 'Employer_Name', 'Work_City', 'Work_State',\n",
      "       'Job_Category', 'Work_State_Code', 'Price_Deflator',\n",
      "       'Offered_Salary_Adjusted', 'Prevailing_Salary_Adjusted', 'Census_2015'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "dsJobsDF_final has 208595 rows and 10 columns\n"
     ]
    }
   ],
   "source": [
    "print(dsJobsDF_final.columns)\n",
    "print(\"\\n\")\n",
    "print(\"dsJobsDF_final has {0} rows and {1} columns\".format(dsJobsDF_final.shape[0], dsJobsDF_final.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready for prime time! We're going to use it to explore [Where the data science jobs are (part2)](https://github.com/sedeh/github.io/blob/master/projects/where_the_data_science_related_jobs_are_part2.ipynb). Before we save the data, let's quickly convert the date string in `Submitted_Date` column to `datetime` and then we can save away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'pandas.tslib.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dsJobsDF_final.Submitted_Date[0]))\n",
    "s = dsJobsDF_final[\"Submitted_Date\"]\n",
    "ts = pd.Series([pd.to_datetime(date_string) for date_string in s])\n",
    "dsJobsDF_final[\"Submitted_Date\"] = ts\n",
    "print(type(dsJobsDF_final.Submitted_Date[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsJobsDF_final.to_csv('dataScienceJobs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time --- 8.932382055123647 minutes ---\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time --- %s minutes ---\" % ((time.time() - total_time) / 60.0))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
