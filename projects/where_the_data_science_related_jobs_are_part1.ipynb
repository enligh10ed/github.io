{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where the data science related jobs are? (Part 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we're going to curate, clean and enrich messy work visa data from the U.S. Department of Labor (DOL). In the next installment, [Where the data science related jobs are (part 2)](https://github.com/sedeh/github.io/blob/master/projects/where_the_data_science_related_jobs_are_part2.ipynb), we'll probe the prepped dataset for insights and trends in data science related jobs, including:\n",
    "\n",
    "- The Top 10 and Bottom 10 States for Data Science Related Jobs\n",
    "- The Top Paying States for Data Science Related Jobs\n",
    "- The Top Paying Companies for Data Science Related Jobs\n",
    "\n",
    "And more.\n",
    "\n",
    "When a U.S. company wants to hire a non-U.S. worker, the company is required to file a work visa (H1B) or permanent residency (greencard) application with DOL. As part of the application, the company must disclose how much it will pay the non-U.S. worker. The company is also required to disclose the `Prevailing Wage` for the job, i.e. how much U.S. workers in the same or similar positions are being paid. The idea is to ensure that hiring non-U.S. workers does not negatively affect the job market for U.S. workers. \n",
    "\n",
    "We'll take advantage of this publicly available data to explore information about data science related jobs. For this analysis, we're going to focus on H1B applications from **2002 to 2015**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the packages we'll need and set things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# For now, let's turn off panda's warning\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "# Snippets like this is used thruout the code to get idea of runtime for different parts of the pipeline\n",
    "total_time = time.time() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get some system information to help with any troubleshooting issues in the likely event you're running this code in a different environment than the one I used. I tested this code on an Amazon EC2 m4.2xlarge instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 :: Anaconda 4.1.1 (64-bit)\n",
      "Linux-3.13.0-92-generic-x86_64-with-debian-jessie-sid\n",
      "cpu cores: 8\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(platform.platform())\n",
    "print(\"cpu cores: {0}\".format(mp.cpu_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python script to download the data is [here](https://github.com/sedeh/github.io/blob/master/resources/h1bdata_download.py). The data is fairly large at **864 mb**. Let's read it in and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "        Submitted_Date      Case_Number                Employer_Name  \\\n",
      "0  2002-01-14 09:46:00  I-02014-0000001             Monsanto Company   \n",
      "1  2002-01-14 09:49:00  I-02014-0000003            Priceline.com LLC   \n",
      "2  2002-01-14 09:58:00  I-02014-0000005     Vestrust Securities L.P.   \n",
      "3  2002-01-14 10:08:00  I-02014-0000007  Cleveland Clinic Foundation   \n",
      "4  2002-01-14 10:08:00  I-02014-0000009      World Data Incorporated   \n",
      "\n",
      "  Employer_City Employer_State Employer_Postal_Code  \\\n",
      "0     St. Louis             MO           63167        \n",
      "1       Norwalk             CT           06954        \n",
      "2  Coral Gables             FL           33134        \n",
      "3     Cleveland             OH           44195        \n",
      "4    Washington             DC           20036        \n",
      "\n",
      "                        Job_Title Approval_Status     Wage_Rate  \\\n",
      "0             MOLECULAR BIOLOGIST       Certified   66700.00000   \n",
      "1     Principal Software Engineer       Certified  123936.00000   \n",
      "2  Associate Sales & Distribution       Certified   40000.00000   \n",
      "3              resident physician       Certified   35995.00000   \n",
      "4         Market Research Analyst       Certified   35608.00000   \n",
      "\n",
      "  Wage_Rate_Unit Part_Time     Work_City Work_State Prevailing_Wage  \n",
      "0           Year         N     ST. LOUIS         MO     38869.00000  \n",
      "1           Year         N       Norwalk         CT     70138.00000  \n",
      "2           Year         N  Coral Gables         FL     35298.00000  \n",
      "3           Year         N     Cleveland         OH     37208.00000  \n",
      "4           Year         N    Washington         DC     37482.00000  \n",
      "\n",
      "\n",
      "h1bdataDF has 5231213 rows and 14 columns\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h1bdataDF = pickle.load(open('h1bdataDF.pkl', 'rb'))\n",
    "print(\"\\n\")\n",
    "print(h1bdataDF.head(5))\n",
    "print(\"\\n\")\n",
    "print(\"h1bdataDF has {0} rows and {1} columns\".format(h1bdataDF.shape[0], h1bdataDF.shape[1]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've about 5 million records. Let's remove any duplicate applications there may be in the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "h1bdataDF2 has 5133079 rows and 14 columns\n"
     ]
    }
   ],
   "source": [
    "h1bdataDF2 = h1bdataDF.drop_duplicates(['Case_Number'], keep='last')\n",
    "print(\"\\n\")\n",
    "print(\"h1bdataDF2 has {0} rows and {1} columns\".format(h1bdataDF2.shape[0], h1bdataDF2.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the available options for `Approval_Status` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Certified', 'Denied', None, '32000.00000', 'Pending', 'Hold',\n",
       "       'Debarred', '', 'roye@fragomen.com', 'omboko@jacksonlewis.com',\n",
       "       'aespiritusanto@fragomen.c', 'mkwok@mltsf.com', 'DENIED',\n",
       "       'CERTIFIED', 'WITHDRAWN', 'CERTIFIED-WITHDRAWN',\n",
       "       'PENDING QUALITY AND COMPLIANCE REVIEW - UNASSIGNED', 'REJECTED',\n",
       "       'INVALIDATED'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1bdataDF.Approval_Status.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purpose, it makes sense to keep only applications that were `certified`. For example, a `denied` application does not give us a lot of confidence about the quality of the application. Before we do that, we need to first remove rows with `None` in `Approval_Status` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filteredDF: (5133076, 14)\n",
      "certifiedDF: (4882580, 14)\n"
     ]
    }
   ],
   "source": [
    "filteredDF = h1bdataDF2.dropna(subset=['Approval_Status'])\n",
    "print(\"filteredDF: {0}\".format(filteredDF.shape))\n",
    "certifiedDF = filteredDF[filteredDF.Approval_Status.str.lower().str.contains('certified')]\n",
    "print(\"certifiedDF: {0}\".format(certifiedDF.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize data science related jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `Job_Title` column to select data science related jobs. Before we start doing anything with that column, let's first make sure it contains only `string`s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Job_Title\n",
      "1780173   66007.9\n",
      "1802719     75483\n",
      "1817997     27.89\n",
      "1839158     71400\n",
      "1891782    541613\n"
     ]
    }
   ],
   "source": [
    "rows_with_strings  = certifiedDF[[\"Job_Title\"]].apply(\n",
    "       lambda row : \n",
    "          any([ isinstance(e, str) for e in row ])\n",
    "       , axis=1) \n",
    "    \n",
    "df_with_no_strings = certifiedDF[[\"Job_Title\"]][~rows_with_strings]\n",
    "print(df_with_no_strings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems some job titles were entered as `float`s. Let's clean out all `float` job titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4882528, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "certifiedDF2 = certifiedDF[rows_with_strings]\n",
    "certifiedDF2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to select and categorize data science related jobs. I chose the categorization criteria based on my view and research of the data science field. A better approach is perhaps to do some pre-clustering to see if interesting patterns emerge around the job titles. There isn't much context/synopsis with the job titles for clustering to give great results. In any case, my categorization criteria isn't perfect and I'm open to your ideas on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsJobsDF shape: (285421, 15)\n",
      "Time to categorize data science related jobs --- 499.87189054489136 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "titles_dict = {'data analyst': 'data analyst', 'business analyst':'business analyst',\n",
    "               'informatic': 'informatics', 'operations research':'operations research',\n",
    "               'data scien': 'data scientist', 'intelligence': 'business intelligence', \n",
    "               'analytics': 'business intelligence', 'data consultan': 'business intelligence', \n",
    "               'data manage': 'business intelligence','reporting & analysis': 'business intelligence',\n",
    "               'data min': 'business intelligence','reporting and analysis': 'business intelligence',\n",
    "               'database analyst':'data architect/engineer', 'data architect': 'data architect/engineer',\n",
    "               'data engineer': 'data architect/engineer','data warehous': 'data architect/engineer',\n",
    "               'database admin': 'data architect/engineer', 'datawarehous': 'data architect/engineer', \n",
    "               'data model': 'data architect/engineer','etl developer':'data architect/engineer', \n",
    "               'market research analyst': 'market analyst', 'market analyst': 'market analyst', \n",
    "               'customer insight': 'market analyst', 'market insight': 'market analyst',\n",
    "               'consumer insight': 'market analyst', 'marketing insight': 'market analyst',\n",
    "               'insights analyst': 'market analyst', 'analytic insight': 'market analyst',\n",
    "               'strategy and insight': 'market analyst', 'mkt. res. analyst': 'market analyst',\n",
    "               'marketing & insights': 'market analyst', 'global insights': 'market analyst',\n",
    "               'insights & analytic':'market analyst', 'strategy and insight': 'market analyst',\n",
    "               'marketing & insight': 'market analyst','insights & analytic':'market analyst',\n",
    "               'health care analyst':'market analyst', 'healthcare analyst': 'market analyst',\n",
    "               'quantitative analyst':'market analyst', 'financial analyst':'market analyst',\n",
    "               'marketing analyst':'market analyst', 'management analyst':'market analyst', \n",
    "               'business development': 'market analyst', 'consumer insight':'market analyst',\n",
    "               'analytic insight':'market analyst', 'data strateg':'market analyst',\n",
    "               'decision scien':'data scientist'}\n",
    "\n",
    "titles_list = []\n",
    "\n",
    "for index, row in certifiedDF2.iterrows():\n",
    "    title = row.Job_Title.lower()\n",
    "    currentTitle = [val for key,val in titles_dict.items() if key in title]\n",
    "    if currentTitle:\n",
    "        titles_list.append(currentTitle[0])\n",
    "    else:\n",
    "        titles_list.append(\"non data science related\")\n",
    "        \n",
    "certifiedDF2[\"Job_Title_revised\"] = titles_list\n",
    "\n",
    "# Filter out non data science jobs\n",
    "dsJobsDF = certifiedDF2[certifiedDF2.Job_Title_revised != \"non data science related\"]\n",
    "dsJobsDF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"dsJobsDF shape: {0}\".format(dsJobsDF.shape))\n",
    "print(\"Time to categorize data science related jobs --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at `Wage_Rate` column and see if any standardization is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year' 'hour' '2 weeks' 'month' 'week' 'bi-weekly' 'yr' 'hr' 'wk' 'mth'\n",
      " 'bi']\n"
     ]
    }
   ],
   "source": [
    "print(dsJobsDF.Wage_Rate_Unit.str.lower().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wage rates range from hourly, weekly, bi-weekly, monthly to yearly. Let's standardize to yearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsJobsDF2: (285413, 15)\n",
      "Time to clean wage columns --- 1.7312259674072266 seconds ---\n",
      "dsJobsDF3: (285413, 17)\n",
      "Time to standardize salary --- 37.64387631416321 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dsJobsDF2 = dsJobsDF\n",
    "\n",
    "# Using regex, remove any special characters such as hyphen or dollar sign in the wage columns.\n",
    "# Such characters are common in columns that contain monetary data such as the wage columns here. \n",
    "# Before applying regex, ensure the columns are of string type.\n",
    "# And remove rows with empty cells in the wage columns.\n",
    "dsJobsDF2['Wage_Rate'] = dsJobsDF2['Wage_Rate'].astype(str)\n",
    "dsJobsDF2['Prevailing_Wage'] = dsJobsDF2['Prevailing_Wage'].astype(str)\n",
    "dsJobsDF2['Wage_Rate'].replace('', np.nan, inplace=True)\n",
    "dsJobsDF2 = dsJobsDF2.dropna(subset=['Wage_Rate'])\n",
    "dsJobsDF2['Prevailing_Wage'].replace('', np.nan, inplace=True)\n",
    "dsJobsDF2 = dsJobsDF2.dropna(subset=['Prevailing_Wage'])\n",
    "\n",
    "# Remove '$' and '-' signs from Wage_Rate column\n",
    "dsJobsDF2['Wage_Rate'] = dsJobsDF2['Wage_Rate'].map(lambda x: re.sub('\\$|-.*', '', x))\n",
    "# Remove '$' and '-' signs from Prevailing_Wage column\n",
    "dsJobsDF2['Prevailing_Wage'] = dsJobsDF2['Prevailing_Wage'].map(lambda x: re.sub('\\$|-.*', '', x))\n",
    "        \n",
    "# Convert the cleaned wage columns to float\n",
    "dsJobsDF2[['Wage_Rate','Prevailing_Wage']] = dsJobsDF2[['Wage_Rate','Prevailing_Wage']].apply(pd.to_numeric)\n",
    "\n",
    "print('dsJobsDF2: {0}'.format(dsJobsDF2.shape))\n",
    "print(\"Time to clean wage columns --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Now we can standardize to yearly\n",
    "start_time = time.time()\n",
    "\n",
    "salary_offered = []\n",
    "salary_prevailing = []\n",
    "for index, row in dsJobsDF2.iterrows():\n",
    "    if row.Wage_Rate_Unit.lower()[0] == 'y':\n",
    "        salary_offered.append(row.Wage_Rate)\n",
    "        salary_prevailing.append(row.Prevailing_Wage)\n",
    "    elif row.Wage_Rate_Unit.lower()[0] == 'm':\n",
    "        salary_offered.append(row.Wage_Rate * 12)\n",
    "        salary_prevailing.append(row.Prevailing_Wage * 12)\n",
    "    elif row.Wage_Rate_Unit.lower() in ['bi', '2 weeks', 'bi-weekly']:\n",
    "        salary_offered.append(row.Wage_Rate * 26) # Assumes 52 work wks/yr\n",
    "        salary_prevailing.append(row.Prevailing_Wage * 26)\n",
    "    elif row.Wage_Rate_Unit.lower()[0] == 'w':\n",
    "        salary_offered.append(row.Wage_Rate * 52) # Assumes 52 work wks/yr\n",
    "        salary_prevailing.append(row.Prevailing_Wage * 52)\n",
    "    else:\n",
    "        salary_offered.append(row.Wage_Rate * 2080) # This handles the hourly rates. Assumes 52 work wks of 40 hrs/wk\n",
    "        salary_prevailing.append(row.Prevailing_Wage * 2080)\n",
    "\n",
    "dsJobsDF3 = dsJobsDF2\n",
    "dsJobsDF3[\"Salary_Offered\"] = salary_offered\n",
    "dsJobsDF3[\"Salary_Prevailing\"] = salary_prevailing\n",
    "\n",
    "# Let's remove any rows without salary Wage information\n",
    "dsJobsDF3 = dsJobsDF3.dropna(subset=['Salary_Offered'])\n",
    "dsJobsDF3 = dsJobsDF3.dropna(subset=['Salary_Prevailing'])\n",
    "\n",
    "print('dsJobsDF3: {0}'.format(dsJobsDF3.shape))\n",
    "dsJobsDF3.reset_index(drop=True, inplace=True) # Resets row index so they start from 0\n",
    "print(\"Time to standardize salary --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some descriptive statistics on `Salary_Offered` column to see the range of salaries we're dealing with here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2.854130e+05\n",
      "mean     1.623086e+05\n",
      "std      4.644013e+06\n",
      "min      1.538000e+01\n",
      "25%      4.761100e+04\n",
      "50%      6.000000e+04\n",
      "75%      7.631500e+04\n",
      "max      1.322880e+09\n",
      "Name: Salary_Offered, dtype: float64\n",
      "\n",
      "\n",
      "60000.0\n"
     ]
    }
   ],
   "source": [
    "print(dsJobsDF3[\"Salary_Offered\"].describe())\n",
    "print(\"\\n\")\n",
    "print(dsJobsDF3[\"Salary_Offered\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, the maximum annual salary is **1.3 billion**! At the other extreme, the minimum is **15.4**. These are likely due to entry errors; we rarely run into people who are paid these kinds of salaries. \n",
    "\n",
    "The median salary, which is **60000.0**, is a reasonable reflection of the center of the distribution. For our analysis, we'll consider any salary as unlilkely (or an `outlier`) if it's less than **23760** or more than **10** times the median. I'm assuming that a data professional is at least making **2** times above the federal poverty line which is at least **11,880** in 2016 (**2** x **11,880** = **23760**). \n",
    "\n",
    "The idea is that we want to consider salaries that are typical for the population. Investigating the outlying values can be a study in and of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283338, 17)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's keep salaries that are greater than 23760 but less than 10 * the median salary\n",
    "med_offered = dsJobsDF3[\"Salary_Offered\"].median()\n",
    "med_prevailing = dsJobsDF3[\"Salary_Prevailing\"].median()\n",
    "dsJobsDF3 = dsJobsDF3[(dsJobsDF3.Salary_Offered > 23760) & (dsJobsDF3.Salary_Offered < 10 * med_offered)]\n",
    "dsJobsDF3 = dsJobsDF3[(dsJobsDF3.Salary_Prevailing > 23760) & (dsJobsDF3.Salary_Prevailing < 10 * med_prevailing)]\n",
    "dsJobsDF3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add price parity information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to adjust salary to reflect regional price parity (inflation). Let's get regional inflation data from the Bureau of Economic Analysis, www.bea.gov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parityDF shape: (285421, 15)\n",
      "Time to read and clean BEA data --- 0.9652659893035889 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "url = 'https://www.bea.gov/newsreleases/regional/rpp/2015/xls/rpp0615.xlsx'\n",
    "response = urlopen(url)\n",
    "parityDF = pd.read_excel(response, skiprows=3)\n",
    "parityDF.drop(parityDF.columns[[1,2,3,4,5,6,9]], axis=1, inplace=True)\n",
    "parityDF.columns = [\"State\", \"2012\", \"2013\"]\n",
    "parityDF['Price_Deflator'] = (parityDF['2012'] + parityDF['2013']) / 2 \n",
    "parityDF.drop(parityDF.columns[[1,2]], axis=1, inplace=True)\n",
    "\n",
    "print(\"parityDF shape: {0}\".format(dsJobsDF.shape))\n",
    "print(\"Time to read and clean BEA data --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the price parity data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State  Price_Deflator\n",
      "0     Alabama           93.65\n",
      "1      Alaska          113.15\n",
      "2     Arizona          103.80\n",
      "3    Arkansas           93.30\n",
      "4  California          120.00\n"
     ]
    }
   ],
   "source": [
    "print(parityDF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to merge the price parity information with the salary data. The common column between both data frames is `State` and we should take a look at `State` columns in both data frames to make sure they match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DC' 'CA' 'MO' 'TX' 'NY' 'IL' 'MN' 'MD' 'NJ' 'MA' 'FL' 'CT' 'WA' 'NC' 'VA'\n",
      " 'PA' 'DE' 'KY' 'GA' 'AZ' 'OH' 'IN' 'TN' 'NH' 'OR' 'MS' 'AL' 'NV' 'MI' 'WI'\n",
      " 'SC' 'CO' 'OK' 'KS' 'LA' 'ID' 'AR' 'IA' 'RI' 'HI' 'UT' 'WY' 'AK' 'WV' 'PW'\n",
      " 'NE' 'PR' 'ND' 'SD' 'VT' 'NM' 'ME' 'MT' 'VI' 'FM' 'AS' 'MH' 'GU' 'MP']\n"
     ]
    }
   ],
   "source": [
    "print(dsJobsDF3.Work_State.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alabama' 'Alaska' 'Arizona' 'Arkansas' 'California' 'Colorado'\n",
      " 'Connecticut' 'Delaware' 'District of Columbia' 'Florida' 'Georgia'\n",
      " 'Hawaii' 'Idaho' 'Illinois' 'Indiana' 'Iowa' 'Kansas' 'Kentucky'\n",
      " 'Louisiana' 'Maine' 'Maryland' 'Massachusetts' 'Michigan' 'Minnesota'\n",
      " 'Mississippi' 'Missouri' 'Montana' 'Nebraska' 'Nevada' 'New Hampshire'\n",
      " 'New Jersey' 'New Mexico' 'New York' 'North Carolina' 'North Dakota'\n",
      " 'Ohio' 'Oklahoma' 'Oregon' 'Pennsylvania' 'Rhode Island' 'South Carolina'\n",
      " 'South Dakota' 'Tennessee' 'Texas' 'Utah' 'Vermont' 'Virginia'\n",
      " 'Washington' 'West Virginia' 'Wisconsin' 'Wyoming' nan 'Maximum' 'Minimum'\n",
      " 'Range' 'Source: U.S. Bureau of Economic Analysis']\n"
     ]
    }
   ],
   "source": [
    "print(parityDF.State.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice two things. First, the price parity dataset contains extraneous rows, `maximum`, `minimum`, `range` and `nan`. Second, the two datasets use different formats for state names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's address the second issue and convert state abbreviation in the work visa dataset to state full name using the list [here](https://raw.githubusercontent.com/sedeh/github.io/master/resources/states.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsJobsDF3: (283323, 17)\n"
     ]
    }
   ],
   "source": [
    "dsJobsDF3 = dsJobsDF3.dropna(subset=['Employer_State'])\n",
    "print('dsJobsDF3: {0}'.format(dsJobsDF3.shape))\n",
    "response = urlopen('https://raw.githubusercontent.com/sedeh/github.io/master/resources/states.txt')\n",
    "lines = response.readlines()\n",
    "state_names_dict = {}\n",
    "for line in lines:\n",
    "    state_code, state_name = line.decode().split(\":\")\n",
    "    state_names_dict[state_code.strip()] = state_name.strip()\n",
    "\n",
    "state_names_list = []\n",
    "\n",
    "for index, row in dsJobsDF3.iterrows():\n",
    "    try:\n",
    "        state_names_list.append(state_names_dict[row.Work_State])\n",
    "    except KeyError:\n",
    "        state_names_list.append(state_names_dict[row.Employer_State])\n",
    "dsJobsDF4 = dsJobsDF3\n",
    "dsJobsDF4[\"Work_State_Code\"] = dsJobsDF4.Work_State\n",
    "dsJobsDF4[\"Work_State\"] = state_names_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now remove the extraneous rows in the price parity data and merge the parity and work visa datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parityDF2: (51, 2)\n",
      "dsJobsDF5: (283323, 20)\n"
     ]
    }
   ],
   "source": [
    "parityDF2 = parityDF[:-5]\n",
    "parityDF2 = parityDF2.dropna(subset=['State'])\n",
    "print('parityDF2: {0}'.format(parityDF2.shape))\n",
    "dsJobsDF5 = dsJobsDF4.merge(parityDF2,how='left', left_on='Work_State', right_on='State')\n",
    "print('dsJobsDF5: {0}'.format(dsJobsDF5.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to use the price parity information to adjust salary to reflect regional cost of living differences. Notice that not all states have price parity information; there is `nan` in the `Price_Deflator` column below. We should replace `nan` with **100.0** so salaries for those states remain unchanged after adjusting for cost of living differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan present\n",
      "[    nan  120.     94.9   102.9   122.75  107.5   104.05  118.1   122.1\n",
      "  114.1   105.35  115.95  110.     97.65  109.8   104.95  108.25   94.75\n",
      "   97.9   103.8    95.35   97.35   96.6   112.75  105.15   92.3    93.65\n",
      "  104.9   100.6    99.2   108.65   95.8    96.65   97.3    99.3    93.3\n",
      "   96.05  104.8   124.35  103.6   102.3   113.15   94.3    96.4    97.1\n",
      "   93.95  107.    101.3   104.4   100.3 ]\n",
      "\n",
      "\n",
      "nan replaced with 100.0\n",
      "[ 100.    120.     94.9   102.9   122.75  107.5   104.05  118.1   122.1\n",
      "  114.1   105.35  115.95  110.     97.65  109.8   104.95  108.25   94.75\n",
      "   97.9   103.8    95.35   97.35   96.6   112.75  105.15   92.3    93.65\n",
      "  104.9   100.6    99.2   108.65   95.8    96.65   97.3    99.3    93.3\n",
      "   96.05  104.8   124.35  103.6   102.3   113.15   94.3    96.4    97.1\n",
      "   93.95  107.    101.3   104.4   100.3 ]\n"
     ]
    }
   ],
   "source": [
    "print('nan present')\n",
    "print(dsJobsDF5.Price_Deflator.unique())\n",
    "print(\"\\n\")\n",
    "where_are_NaNs = np.isnan(dsJobsDF5.Price_Deflator)\n",
    "dsJobsDF5.Price_Deflator[where_are_NaNs] = 100.0\n",
    "print('nan replaced with 100.0')\n",
    "print(dsJobsDF5.Price_Deflator.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to adjust salary with the price parity information. This isn't perfect because we're using price parity information from **2012 and 2013** to adjust salary information from **2002 to 2015**. However, it seems likely that inflation shows consistent trend from year to year among states. For e.g., New York has been more expensive to live in than Iowa for as long as we can remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to adjust salary for inflation --- 59.78936195373535 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "salary_offered_adjusted = []\n",
    "salary_prevailing_adjusted = []\n",
    "\n",
    "for index, row in dsJobsDF5.iterrows():\n",
    "    salary_offered_adjusted.append(row.loc['Salary_Offered'] / (row.loc['Price_Deflator'] / 100))\n",
    "    salary_prevailing_adjusted.append(row.loc['Salary_Prevailing'] / (row.loc['Price_Deflator'] / 100))\n",
    "    \n",
    "dsJobsDF6 = dsJobsDF5\n",
    "dsJobsDF6[\"Offered_Salary_Adjusted\"] = salary_offered_adjusted\n",
    "dsJobsDF6[\"Prevailing_Salary_Adjusted\"] = salary_prevailing_adjusted\n",
    "\n",
    "print(\"Time to adjust salary for inflation --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add population information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply knowing that California has **30,000** data science related jobs compared to say **3500** for Alaska really does not tell us all that much. Ideally, we want relative truth where we can get a sense of the number of H1B jobs per capita. So let's add population information from census.gov to help us do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         State  Census_2015\n",
      "0     .Alabama    4858979.0\n",
      "1      .Alaska     738432.0\n",
      "2     .Arizona    6828065.0\n",
      "3    .Arkansas    2978204.0\n",
      "4  .California   39144818.0\n",
      "\n",
      "\n",
      "        State  Census_2015\n",
      "0     Alabama    4858979.0\n",
      "1      Alaska     738432.0\n",
      "2     Arizona    6828065.0\n",
      "3    Arkansas    2978204.0\n",
      "4  California   39144818.0\n",
      "\n",
      "\n",
      "censusDF: (59, 2)\n",
      "Time to read and clean census data --- 0.40192627906799316 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "url = 'http://www.census.gov/popest/data/state/totals/2015/tables/NST-EST2015-01.xlsx'\n",
    "response = urlopen(url)\n",
    "xl = pd.ExcelFile(response)\n",
    "sheet_names = xl.sheet_names\n",
    "DF = xl.parse(sheet_names[0], skiprows=8)\n",
    "DF.columns = [\"State\", \"Census\", \"Estimates Base\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"Census_2015\"]\n",
    "censusDF = DF[[\"State\", \"Census_2015\"]]\n",
    "censusDF = censusDF.iloc[0:51,]\n",
    "print(censusDF.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Remove dot sign from the State column\n",
    "censusDF['State'] = censusDF['State'].map(lambda x: re.sub('\\.', '', x))\n",
    "censusDF['State'] = censusDF['State'].str.title()\n",
    "\n",
    "# Add population of US territories not included in the data source\n",
    "temp = pd.DataFrame([['Puerto Rico', 3474182],\n",
    "                   ['Guam', 159358],\n",
    "                   ['Virgin Islands', 106405],\n",
    "                   ['American Samoa', 55519],\n",
    "                   ['Northern Mariana Islands', 53883],\n",
    "                   ['Palau', 20918],\n",
    "                   ['Federated States Of Micronesia', 103549],\n",
    "                   ['Marshall Islands', 52634]], columns=[\"State\", \"Census_2015\"])\n",
    "censusDF = censusDF.append(temp, ignore_index=True)\n",
    "print(censusDF.head())\n",
    "print(\"\\n\")\n",
    "print(\"censusDF: {0}\".format(censusDF.shape))\n",
    "print(\"Time to read and clean census data --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the census data to our work visa data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283323, 24)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsJobsDF7 = dsJobsDF6.merge(censusDF,how='left', left_on='Work_State', right_on='State')\n",
    "dsJobsDF7.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can drop columns we don't need. Here are the columns in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Submitted_Date', 'Case_Number', 'Employer_Name', 'Employer_City',\n",
       "       'Employer_State', 'Employer_Postal_Code', 'Job_Title',\n",
       "       'Approval_Status', 'Wage_Rate', 'Wage_Rate_Unit', 'Part_Time',\n",
       "       'Work_City', 'Work_State', 'Prevailing_Wage', 'Job_Title_revised',\n",
       "       'Salary_Offered', 'Salary_Prevailing', 'Work_State_Code', 'State_x',\n",
       "       'Price_Deflator', 'Offered_Salary_Adjusted',\n",
       "       'Prevailing_Salary_Adjusted', 'State_y', 'Census_2015'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsJobsDF7.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsJobsDF_final = dsJobsDF7.drop(dsJobsDF7.columns[[1,3,4,5,6,7,8,9,10,13,15,16,18,22]], axis=1)\n",
    "dsJobsDF_final.rename(columns={'Job_Title_revised':'Job_Category'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our final dataset after dropping some columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Submitted_Date', 'Employer_Name', 'Work_City', 'Work_State',\n",
      "       'Job_Category', 'Work_State_Code', 'Price_Deflator',\n",
      "       'Offered_Salary_Adjusted', 'Prevailing_Salary_Adjusted', 'Census_2015'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "dsJobsDF_final has 283323 rows and 10 columns\n"
     ]
    }
   ],
   "source": [
    "print(dsJobsDF_final.columns)\n",
    "print(\"\\n\")\n",
    "print(\"dsJobsDF_final has {0} rows and {1} columns\".format(dsJobsDF_final.shape[0], dsJobsDF_final.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready for prime time! We're going to explore it in [Where the data science related jobs are (part2)](https://github.com/sedeh/github.io/blob/master/projects/where_the_data_science_related_jobs_are_part2.ipynb). Before we save the data, let's quickly convert the date string in `Submitted_Date` column to `datetime` and then we can save away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'pandas.tslib.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dsJobsDF_final.Submitted_Date[0]))\n",
    "s = dsJobsDF_final[\"Submitted_Date\"]\n",
    "ts = pd.Series([pd.to_datetime(date_string) for date_string in s])\n",
    "dsJobsDF_final[\"Submitted_Date\"] = ts\n",
    "print(type(dsJobsDF_final.Submitted_Date[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsJobsDF_final.to_csv('dataScienceJobs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time --- 12.587802549203237 minutes ---\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time --- %s minutes ---\" % ((time.time() - total_time) / 60.0))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
